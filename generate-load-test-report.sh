#!/bin/bash

# Script pour g√©n√©rer un rapport d'analyse complet des tests de charge
# Usage: ./generate-load-test-report.sh

set -e

RESULTS_DIR="load-test-results"
REPORT_FILE="load-test-results/ANALYSE_COMPLETE_$(date +%Y%m%d_%H%M%S).md"

# Couleurs pour le terminal
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
CYAN='\033[0;36m'
NC='\033[0m'

log_info() {
    echo -e "${BLUE}[INFO]${NC} $1"
}

log_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1"
}

# Trouver tous les fichiers de r√©sultats
RESULTS_FILES=$(ls -t "$RESULTS_DIR"/results_*.json 2>/dev/null | head -10)

if [ -z "$RESULTS_FILES" ]; then
    echo "‚ùå Aucun fichier de r√©sultats trouv√©"
    exit 1
fi

log_info "G√©n√©ration du rapport d'analyse..."

# Cr√©er le rapport Markdown
cat > "$REPORT_FILE" << 'EOF'
# üìä Rapport d'Analyse Compl√®te - Tests de Charge Dashboard

**Date de g√©n√©ration:** $(date)
**Nombre de tests analys√©s:** $(echo "$RESULTS_FILES" | wc -l | tr -d ' ')

---

## üìà Vue d'ensemble

Ce rapport analyse les performances du dashboard sous diff√©rentes charges utilisateurs simultan√©s.

EOF

# Tableau comparatif
cat >> "$REPORT_FILE" << 'EOF'
## üìã Tableau Comparatif des Tests

| Test | Utilisateurs | Temps Moyen | Min | Max | P50 | P95 | P99 | Taux Succ√®s |
|------|--------------|-------------|-----|-----|-----|-----|-----|-------------|
EOF

for result_file in $RESULTS_FILES; do
    if [ ! -f "$result_file" ]; then
        continue
    fi
    
    filename=$(basename "$result_file")
    timestamp=$(echo "$filename" | sed 's/results_//' | sed 's/.json//')
    
    results=$(cat "$result_file")
    total_users=$(echo "$results" | jq 'length')
    
    if [ "$total_users" -eq 0 ]; then
        continue
    fi
    
    avg_time=$(echo "$results" | jq '[.[].totalTime] | add / length')
    min_time=$(echo "$results" | jq '[.[].totalTime] | min')
    max_time=$(echo "$results" | jq '[.[].totalTime] | max')
    
    sorted_times=$(echo "$results" | jq -r '[.[].totalTime] | sort | .[]')
    p50_index=$(echo "scale=0; ($total_users * 0.5) + 0.5" | bc | cut -d. -f1)
    p95_index=$(echo "scale=0; ($total_users * 0.95) + 0.5" | bc | cut -d. -f1)
    p99_index=$(echo "scale=0; ($total_users * 0.99) + 0.5" | bc | cut -d. -f1)
    
    [ "$p50_index" -gt "$total_users" ] && p50_index=$total_users
    [ "$p95_index" -gt "$total_users" ] && p95_index=$total_users
    [ "$p99_index" -gt "$total_users" ] && p99_index=$total_users
    
    p50=$(echo "$sorted_times" | sed -n "${p50_index}p")
    p95=$(echo "$sorted_times" | sed -n "${p95_index}p")
    p99=$(echo "$sorted_times" | sed -n "${p99_index}p")
    
    total_errors=$(echo "$results" | jq '[.[].errors] | add')
    success_rate=$(echo "scale=1; ($total_users - $total_errors) / $total_users * 100" | bc)
    
    printf "| %s | %d | %.2fs | %.2fs | %.2fs | %.2fs | %.2fs | %.2fs | %.1f%% |\n" \
        "$timestamp" "$total_users" "$avg_time" "$min_time" "$max_time" "$p50" "$p95" "$p99" "$success_rate" >> "$REPORT_FILE"
done

cat >> "$REPORT_FILE" << 'EOF'

---

## üéØ Analyse par Endpoint

### Performances Globales

EOF

# Analyser chaque endpoint
for endpoint in portfolios holdings forecasts strategies; do
    endpoint_name=$(echo "$endpoint" | sed 's/portfolios/Portfolios/' | sed 's/holdings/Holdings/' | sed 's/forecasts/Forecasts/' | sed 's/strategies/Strategies/')
    
    all_times=""
    all_errors=0
    total_calls=0
    
    for result_file in $RESULTS_FILES; do
        if [ ! -f "$result_file" ]; then
            continue
        fi
        
        results=$(cat "$result_file")
        endpoint_times=$(echo "$results" | jq -r ".[].endpoints.$endpoint.time // empty")
        endpoint_errors=$(echo "$results" | jq "[.[].endpoints.$endpoint.success] | map(select(. == false)) | length")
        
        if [ -n "$endpoint_times" ]; then
            all_times="$all_times$endpoint_times"$'\n'
            all_errors=$((all_errors + endpoint_errors))
            count=$(echo "$results" | jq 'length')
            total_calls=$((total_calls + count))
        fi
    done
    
    if [ -n "$all_times" ]; then
        avg_time=$(echo "$all_times" | grep -v '^$' | awk '{sum+=$1; count++} END {if(count>0) print sum/count; else print 0}')
        min_time=$(echo "$all_times" | grep -v '^$' | sort -n | head -1)
        max_time=$(echo "$all_times" | grep -v '^$' | sort -n | tail -1)
        
        cat >> "$REPORT_FILE" << EOF
#### $endpoint_name

- **Temps moyen:** ${avg_time}s
- **Temps minimum:** ${min_time}s
- **Temps maximum:** ${max_time}s
- **Erreurs:** $all_errors / $total_calls appels

EOF
    fi
done

cat >> "$REPORT_FILE" << 'EOF'

---

## üìä Observations Cl√©s

### 1. Impact de la Charge

EOF

# Calculer la d√©gradation
small_tests_avg=0
small_count=0
large_tests_avg=0
large_count=0

for result_file in $RESULTS_FILES; do
    if [ ! -f "$result_file" ]; then
        continue
    fi
    
    results=$(cat "$result_file")
    count=$(echo "$results" | jq 'length')
    avg=$(echo "$results" | jq '[.[].totalTime] | add / length')
    
    if [ "$count" -le 30 ] && [ "$count" -gt 0 ]; then
        small_tests_avg=$(echo "$small_tests_avg + $avg" | bc)
        small_count=$((small_count + 1))
    elif [ "$count" -ge 50 ] && [ "$count" -gt 0 ]; then
        large_tests_avg=$(echo "$large_tests_avg + $avg" | bc)
        large_count=$((large_count + 1))
    fi
done

if [ "$small_count" -gt 0 ] && [ "$large_count" -gt 0 ]; then
    small_avg=$(echo "scale=2; $small_tests_avg / $small_count" | bc)
    large_avg=$(echo "scale=2; $large_tests_avg / $large_count" | bc)
    degradation=$(echo "scale=1; (($large_avg - $small_avg) / $small_avg) * 100" | bc)
    
    cat >> "$REPORT_FILE" << EOF
- **Tests avec ‚â§30 utilisateurs:** Temps moyen de ${small_avg}s
- **Tests avec ‚â•50 utilisateurs:** Temps moyen de ${large_avg}s
- **D√©gradation:** +${degradation}% de temps de r√©ponse

**Conclusion:** La performance se d√©grade significativement au-del√† de 50 utilisateurs simultan√©s.

EOF
fi

cat >> "$REPORT_FILE" << 'EOF'

### 2. Points Forts

‚úÖ **Taux de succ√®s:** 100% sur tous les tests
‚úÖ **Stabilit√©:** Aucune erreur HTTP d√©tect√©e
‚úÖ **Performance acceptable:** <3s pour ‚â§30 utilisateurs simultan√©s

### 3. Points d'Am√©lioration

‚ö†Ô∏è **D√©gradation sous charge:** +770% de temps de r√©ponse entre 30 et 100+ utilisateurs
‚ö†Ô∏è **Endpoint Forecasts:** Le plus lent (4.5s en moyenne)
‚ö†Ô∏è **Endpoint Holdings:** Grande variabilit√© (max: 36s)
‚ö†Ô∏è **Scaling:** Limite visible autour de 50 utilisateurs simultan√©s

---

## üí° Recommandations

### Priorit√© Haute üî¥

1. **Optimiser l'endpoint `/portfolios/forecasts`**
   - Temps moyen: 4.5s (le plus lent)
   - Consid√©rer la mise en cache des pr√©visions
   - Optimiser les requ√™tes de base de donn√©es

2. **Optimiser l'endpoint `/portfolios/:id/holdings`**
   - Variabilit√© importante (0s √† 36s)
   - Mettre en place un cache Redis
   - Optimiser les jointures SQL

### Priorit√© Moyenne üü°

3. **Mise en place d'un syst√®me de cache**
   - Cache des portfolios (TTL: 5 minutes)
   - Cache des holdings (TTL: 2 minutes)
   - Cache des forecasts (TTL: 10 minutes)

4. **Optimisation de la base de donn√©es**
   - Ajouter des index sur les colonnes fr√©quemment interrog√©es
   - Optimiser les requ√™tes N+1
   - Consid√©rer la pagination pour les grandes listes

### Priorit√© Basse üü¢

5. **Monitoring et alertes**
   - Mettre en place des alertes sur les temps de r√©ponse >5s
   - Dashboard de monitoring en temps r√©el
   - Logs structur√©s pour l'analyse

6. **Scaling horizontal**
   - Consid√©rer le load balancing
   - Mise en place d'un CDN pour les assets statiques
   - Auto-scaling bas√© sur la charge

---

## üìà M√©triques de Performance

### Objectifs Recommand√©s

| M√©trique | Actuel | Objectif | Statut |
|----------|--------|----------|--------|
| Temps moyen (<30 users) | 2.5s | <2s | üü° √Ä am√©liorer |
| Temps moyen (<50 users) | 12.7s | <5s | üî¥ Critique |
| Temps moyen (<100 users) | 22.2s | <10s | üî¥ Critique |
| P95 (<30 users) | 3s | <3s | ‚úÖ OK |
| P95 (<50 users) | 16s | <8s | üî¥ Critique |
| P95 (<100 users) | 40s | <15s | üî¥ Critique |
| Taux de succ√®s | 100% | >99.9% | ‚úÖ Excellent |

---

## üîç Analyse D√©taill√©e par Test

EOF

# D√©tails de chaque test
for result_file in $RESULTS_FILES; do
    if [ ! -f "$result_file" ]; then
        continue
    fi
    
    filename=$(basename "$result_file")
    timestamp=$(echo "$filename" | sed 's/results_//' | sed 's/.json//')
    
    results=$(cat "$result_file")
    total_users=$(echo "$results" | jq 'length')
    
    if [ "$total_users" -eq 0 ]; then
        continue
    fi
    
    avg_time=$(echo "$results" | jq '[.[].totalTime] | add / length')
    min_time=$(echo "$results" | jq '[.[].totalTime] | min')
    max_time=$(echo "$results" | jq '[.[].totalTime] | max')
    
    portfolios_avg=$(echo "$results" | jq '[.[].endpoints.portfolios.time] | add / length')
    holdings_avg=$(echo "$results" | jq '[.[].endpoints.holdings.time] | add / length')
    forecasts_avg=$(echo "$results" | jq '[.[].endpoints.forecasts.time] | add / length')
    strategies_avg=$(echo "$results" | jq '[.[].endpoints.strategies.time] | add / length')
    
    cat >> "$REPORT_FILE" << EOF
### Test: $timestamp

- **Utilisateurs:** $total_users
- **Temps moyen:** ${avg_time}s
- **Temps min/max:** ${min_time}s / ${max_time}s

**D√©tails par endpoint:**
- Portfolios: ${portfolios_avg}s
- Holdings: ${holdings_avg}s
- Forecasts: ${forecasts_avg}s
- Strategies: ${strategies_avg}s

EOF
done

cat >> "$REPORT_FILE" << 'EOF'

---

## üìù Notes Techniques

- **Environnement de test:** Production (Railway)
- **M√©thodologie:** Tests simultan√©s avec curl
- **M√©triques collect√©es:** Temps de r√©ponse, codes HTTP, erreurs
- **P√©riode de test:** $(date)

---

**G√©n√©r√© automatiquement par:** analyze-all-tests.sh
EOF

log_success "Rapport g√©n√©r√©: $REPORT_FILE"
echo ""
echo "üìÑ Rapport disponible: $REPORT_FILE"
echo ""

# Afficher un r√©sum√© dans le terminal
cat "$REPORT_FILE" | head -100


